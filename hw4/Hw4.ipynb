{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e4f906c-b6cb-499f-96bf-9492e2042ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import csv\n",
    "import random\n",
    "from collections import Counter\n",
    "import string\n",
    "from torch.utils.data import Dataset, random_split, DataLoader\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import time\n",
    "from torch.distributions.categorical import Categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a3261c-2606-446a-ab8a-9a380279394f",
   "metadata": {},
   "source": [
    "## Preprocessing & IDA\n",
    "Dataset: https://www.kaggle.com/datasets/carlosgdcj/genius-song-lyrics-with-language-information\n",
    "This dataset contains 9GiB of lyrical data of all different genres and languages. I am going to focus on pop music. I would like to create a \n",
    "generative pop lyrics model using this dataset. To do so I will train a RNN,LSTM,GRU and compare the ending results. I started by reducing this 9GiB file to around 1.7 GiB of lyrics which are only english pop music."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4b7a921b-2f38-4d2a-aa63-52cd07e41c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DSET = \"pop.csv\"\n",
    "f = open(DSET)\n",
    "csv_r = csv.reader(f)\n",
    "DATA = list(csv_r)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7a94c18a-7d48-4e05-98fd-fd6acd39f9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer():\n",
    "    def __init__(self, DATA, min_app, file=None):\n",
    "        self.tokens = None\n",
    "        self.token_map = None\n",
    "        if file == None:\n",
    "            self.create_tokens(DATA, min_app)\n",
    "        else:\n",
    "            self.load_dict(file)\n",
    "\n",
    "    def load_dict(self, file):\n",
    "        f = open(file, 'r')\n",
    "        lines = f.readlines()\n",
    "        f.close()\n",
    "        lines = [line.strip() for line in lines]\n",
    "        self.tokens = lines\n",
    "        self.token_map = {line:i for i, line in enumerate(lines)}\n",
    "        \n",
    "    def process_string(self, lyrics):\n",
    "        lyrics.translate(str.maketrans('', '', string.punctuation))\n",
    "        return lyrics.lower().split()\n",
    "        \n",
    "    def create_tokens(self, D, min_app):\n",
    "        wc = Counter()\n",
    "        for row in D:\n",
    "            lyrics = row[6]\n",
    "            wc.update(self.process_string(lyrics))\n",
    "        wc = dict(filter(lambda x: int(x[1]) > min_app, wc.items()))\n",
    "        self.tokens = [\"<SOS>\", \"<EOS>\", \"<PAD>\", \"<UNK>\"] + list(wc.keys())\n",
    "        self.token_map = {token:i for i, token in enumerate(self.tokens)}\n",
    "        \n",
    "    def get_token(self, word):\n",
    "        if word in self.token_map:\n",
    "            return self.token_map[word]\n",
    "        return self.token_map[\"<UNK>\"]\n",
    "\n",
    "    def tokenize(self, lyrics):\n",
    "        words = self.process_string(lyrics)\n",
    "        return [self.get_token(word) for word in words]\n",
    "\n",
    "    def stringify(self, tokens):\n",
    "        return [self.tokens[i] for i in tokens]\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fb400ae4-fd41-4761-ac40-d9de57472d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45509\n",
      "['<SOS>', '<EOS>', '<PAD>', '<UNK>', '[chorus]', 'what', 'are', 'words', 'worth?', '[verse', '1]', 'in', 'papers,', 'books', 'on', 'tv,', 'for', 'crooks', 'of', 'comfort,', 'peace', 'to', 'make', 'the', 'fighting', 'cease', 'tell', 'you', 'do', 'working', 'hard', 'eat', 'your', 'but', \"don't\", 'go', 'hungry', 'have', 'always', 'nearly', 'hung', 'me', 'a', 'ram', 'sam', 'sam,', 'hi', 'yay,', 'yippie', 'yi', 'yay', 'awoo', 'ayee', '2]', 'skill', 'and', 'romance', 'thrill', 'stupid,', 'fun', 'can', 'put', 'run', 'mots', 'qui', 'la', 'le', 'fruit', '3]', \"it's\", 'rap', 'race,', 'with', 'fast', 'pace', 'concrete', 'words,', 'abstract', 'crazy', 'lying', 'hazy', 'dying', 'faith', 'straight', 'rare', 'swear', 'good', 'bad', '4]', 'pay', 'four-letter', 'i', 'cannot', 'say', 'toilet,', 'dirty', 'devil', 'trouble,', 'subtle', 'anger,']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(None, 100, \"vocab.vocab\")\n",
    "print(len(tokenizer.tokens))\n",
    "print(tokenizer.tokens[:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "957b8a86-0c10-4a62-8e4c-1b02a807c41c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23, 1509, 3399, 4974, 2998, 101, 23, 91, 5338, 23, 3697]\n",
      "['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'i', 'forgot', 'the', 'rest']\n"
     ]
    }
   ],
   "source": [
    "toks = tokenizer.tokenize(\"The quick brown fox jumped over the i forgot the rest\")\n",
    "print(toks)\n",
    "print(tokenizer.stringify(toks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5c464449-69b2-443c-8a94-553c59d617c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LyricSet(Dataset):\n",
    "    def __init__(self, data, tokenizer, sequence_size):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.sequence_size = sequence_size\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.tokenizer.tokenize(self.data[idx][6])\n",
    "        start_idx = 0 if len(sequence) <= self.sequence_size else random.randint(0, len(sequence) - self.sequence_size-1)\n",
    "        sequence = sequence[start_idx:]\n",
    "        if len(sequence) < self.sequence_size-2:\n",
    "            out =  torch.tensor([tokenizer.get_token(\"<SOS>\")] + sequence + [tokenizer.get_token(\"<EOS>\")] + \n",
    "                                [tokenizer.get_token(\"<PAD>\")]*(self.sequence_size-len(sequence)-2), dtype=torch.long)\n",
    "            assert out.shape[0] == self.sequence_size, str(out.shape) + \" \" + str(len(sequence))\n",
    "        else :\n",
    "            out =  torch.tensor([tokenizer.get_token(\"<SOS>\")] + sequence[:self.sequence_size-2] + [tokenizer.get_token(\"<EOS>\")], \n",
    "                                dtype=torch.long)\n",
    "            assert out.shape[0] == self.sequence_size, str(out.shape)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "371c6d63-9685-4be4-9766-70ca474c1755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  0,   7, 113, 114, 115, 116, 117,  21,  23, 118, 119, 120,  93,  23,\n",
      "        116, 121,  22,  42, 122,   7,   6, 112,  42, 123, 124, 125, 126,  93,\n",
      "          5, 127, 128,  34, 128,   5, 127,  93,  72,  42, 129,  70, 130,  55,\n",
      "         42, 129,  70, 104, 130,  42, 129, 104,  42,  70, 106,  42, 129,  70,\n",
      "         70, 131, 132,  16,  23, 133,  86, 134, 135, 136,  23, 137,  69, 138,\n",
      "        139,   3, 140, 141,  42,   3,   3, 138, 142,   4,  42,  43,  44,  45,\n",
      "         42,  43,  44,  44,   3,   3,   3,   3,   3,  43,  44,  44,  46,   3,\n",
      "         47,   1])\n",
      "torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "dset = LyricSet(DATA, tokenizer, 100)\n",
    "print(dset[0])\n",
    "print(dset[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fbd9dca7-760d-4241-976c-08c946ebd79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, val_set, test_set = random_split(dset, [0.9, 0.05, 0.05])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8761341b-204d-4156-856c-c78cda2b5aa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.488923022434901"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference = [['this', 'is', 'ae', 'test','rest','pep','did']]\n",
    "candidate = ['this', 'is', 'ad', 'test','rest','pep','did']\n",
    "sentence_bleu(reference, candidate) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "da014ec0-7914-4695-95df-496f07684982",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.392814650900513"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_1 =    list(\"the quick brown fox is really fast and jumps high\".split())\n",
    "reference = [list(\"the brown fox is pretty fast and jumps high\".split())]\n",
    "sentence_bleu(reference, test_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3776cc02-67c3-4e36-9488-6a09ef352a1b",
   "metadata": {},
   "source": [
    "## Model Creation\n",
    "We will begin with the an RNN model then progress into more \"complicated\" models which are expected to preform better. Our guiding metric\n",
    "will be the bleu score, This will also serve as source of the pre-trained embeddings for the later part of this assignment. Natural Language is inherently a time / order dependent sequence, with different permutations having different meanings. This problem could not be solved a non temporal model as a result of this. Time would have to be encoded into the NN in some way which would be really difficult for this specific task as each time point is also a vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ce84c44c-935d-4f29-b6d7-721c0e740d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LyricalGenius(nn.Module):\n",
    "    def __init__(self, n_toks, embedding_size, dim_hidden, n_rnn_layers, device=None):\n",
    "        super().__init__()\n",
    "        self.dim_hidden = dim_hidden\n",
    "        self.n_tok = n_toks\n",
    "        self.emb = nn.Embedding(n_toks, embedding_size,device=device)\n",
    "        self.rnn = nn.RNN(input_size=embedding_size, hidden_size=dim_hidden, num_layers=n_rnn_layers,\n",
    "                            batch_first=True, device=device)\n",
    "        self.out = nn.Sequential(*[nn.Linear(dim_hidden, dim_hidden*2, device=device), nn.ReLU(), nn.Linear(dim_hidden*2, n_toks, device=device)\n",
    "                                   , nn.LogSoftmax(dim=1)])\n",
    "    def forward(self, seq):\n",
    "        b, n = seq.shape\n",
    "        seq = self.emb(seq)\n",
    "        seq, _  = self.rnn(seq)\n",
    "        seq = self.out(seq.reshape(-1, self.dim_hidden))\n",
    "        return seq.reshape(b, n, self.n_tok)\n",
    "\n",
    "class LyricalGenius2(nn.Module):\n",
    "    def __init__(self, n_toks, embedding_size, dim_hidden, n_rnn_layers, device=None):\n",
    "        super().__init__()\n",
    "        self.dim_hidden = dim_hidden\n",
    "        self.n_tok = n_toks\n",
    "        self.emb = nn.Embedding(n_toks, embedding_size,device=device)\n",
    "        self.rnn = nn.LSTM(input_size=embedding_size, hidden_size=dim_hidden, num_layers=n_rnn_layers,\n",
    "                            batch_first=True, device=device)\n",
    "        self.out = nn.Sequential(*[nn.Linear(dim_hidden, dim_hidden*2, device=device), nn.ReLU(), nn.Linear(dim_hidden*2, n_toks, device=device)\n",
    "                                   , nn.LogSoftmax(dim=1)])\n",
    "    def forward(self, seq):\n",
    "        b, n = seq.shape\n",
    "        seq = self.emb(seq)\n",
    "        seq, _  = self.rnn(seq)\n",
    "        seq = self.out(seq.reshape(-1, self.dim_hidden))\n",
    "        return seq.reshape(b, n, self.n_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "369ba003-2e91-4253-9bac-b4f0e35d12a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = LyricalGenius(len(tokenizer.tokens), 256, 1024, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3d2ba18c-1b72-47cb-9b45-b86826c510fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 100, 45509])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = dset[0].unsqueeze(0)\n",
    "test_model(test).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "57dd904f-8d71-4443-b5bb-75068dd93500",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, optim, loss, model, train_loader, val_loader, metric_fn, device):\n",
    "    for e in range(epochs):\n",
    "        running_loss = 0\n",
    "        for i, seq in enumerate(train_loader):\n",
    "            optim.zero_grad()\n",
    "            X = seq[:, :-1].to(device)\n",
    "            Y = seq[:, 1: ].to(device)\n",
    "            pred = model(X)\n",
    "            pred = pred.reshape(-1, model.n_tok)\n",
    "            Y = Y.reshape(-1)\n",
    "            l = loss(pred, Y)\n",
    "            running_loss +=l.item()\n",
    "            l.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "            optim.step()\n",
    "            if i % 1000 == 0 and i!=0:\n",
    "                print(\"Epoch\", e, \"iteration\", i, \"Loss:\", running_loss/1000)\n",
    "                running_loss = 0\n",
    "        print(\"Epoch\", e, \"metric:\", metric_fn(model, val_loader, device))\n",
    "    return model\n",
    "\n",
    "def BLEU(model, loader, device):\n",
    "    model.eval()\n",
    "    batch = next(iter(loader)).to(device)\n",
    "    X = batch[:, :-1]\n",
    "    Y = batch[:, 1:].tolist()\n",
    "    predictions = torch.argmax(model(batch), dim=-1)\n",
    "    scores = []\n",
    "    for i in range(batch.shape[0]):\n",
    "        ex = tokenizer.stringify(predictions[i, :].squeeze(0).tolist())\n",
    "        reference = [tokenizer.stringify(Y[i])]\n",
    "        scores.append(sentence_bleu(reference, ex))\n",
    "    model.train()\n",
    "    return sum(scores)/len(scores)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bb9f7a02-e133-400d-a624-9368fc9f7d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "writer = LyricalGenius2(len(tokenizer.tokens), 64, 256, 3, device=device)\n",
    "NLL = nn.NLLLoss(ignore_index=tokenizer.get_token(\"<PAD>\"))\n",
    "batch_size = 16\n",
    "train_loader, val_loader, train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0), \\\n",
    "                                         DataLoader(val_set, batch_size=batch_size, shuffle=True, num_workers=0),   \\\n",
    "                                         DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "#writer = LyricalGenius(len(tokenizer.tokens), 64, 256, 3, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "3ee2c90a-88cb-467b-9b89-00f5882bca00",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.Adam(lr=5e-4, params=writer.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1004ffc7-2376-4557-8e6c-4be51cf8f050",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = train(3, optim, NLL, writer, train_loader, val_loader, BLEU, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "d4ac7baf-0a5b-4e2b-8156-a78df73b717c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(writer.state_dict(), \"LSTM.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "24082a02-bd8d-4b52-ab86-389c8f417dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu = torch.device('cpu')\n",
    "def generate_response(starting_sequence, model):\n",
    "    seq = tokenizer.tokenize(starting_sequence)\n",
    "    next_token = -1\n",
    "    while next_token != tokenizer.get_token(\"<EOS>\"):\n",
    "        if len(seq) == 50:\n",
    "            break\n",
    "        X = torch.tensor([seq], dtype=torch.long).to(device)\n",
    "        pred = model(X)[:, -1].reshape(-1)\n",
    "        distrib = Categorical(logits=pred)\n",
    "        next_token = distrib.sample()\n",
    "        seq.append(next_token)\n",
    "    print(tokenizer.stringify(seq))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "cc997380-a74f-415d-bd91-5dcd536bebe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45509\n",
      "['i', 'know', 'that', 'looked', 'said', 'how', 'bad', 'i', 'could', 'with', 'you,', 'something', 'new', 'maybe', 'free', 'to', 'be', 'surprised', 'seeing', 'me', 'lend', '<EOS>']\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenizer.tokens))\n",
    "generate_response(\"i know\", writer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b36bb5-7413-419c-b891-0e3129e5ab7f",
   "metadata": {},
   "source": [
    "## Comparisons \n",
    "The quality of the output from the LSTM model is somewhat clear, It is also more clear from the value of the loss function / convergence properties of the model. The LSTM quickly surpasses the RNN's loss value and with a learning rate that is signifigantly higher. The main advantages of the LSTM/GRU over the regular RNN is the inductive basis' baked into these models. By specifying exactly how we want the model to update its weights, (given that this update scheme is relatively good) the model doesn't have to learn these mechanisms on its own. For example with the CNN, their is the inductive bais of locality and weight sharing. These inductive bias' result in temporal models which converge much faster, which is a really big help considering how difficult it is for these models to train in the first place as a result of\n",
    "vanishing/exploding gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cb0386-abd1-4a35-92ee-97481519fd67",
   "metadata": {},
   "source": [
    "## Simularities\n",
    "Here is my implementation of two simularity/disimularity metrics. The simularity metric I used is the cosine simularity. When you maximize the output of this functin with respect to the second vector you get the most \"simular\" vectors. This is done by calculating the cosine\n",
    "of the angle between the two vectors. The dissimularity metric I use is euclidean distance. When you maximize the dissimulairty metric you get a vector that is very \"dissimular\" from the first vector. Note that since the pre-trained vectors are a result of pop-lyrics. This can be seen clearly as the most simular word to love is hate which checks out when it comes to the lyrics of pop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "af921173-0435-4cf0-9a10-9569b2c68f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SIMULARITY METRIC\n",
    "def cosine_simularity(vec_1, vec_2):\n",
    "    return torch.dot(vec_1, vec_2)/(torch.norm(vec_1) * torch.norm(vec_2))\n",
    "#DISSIMULARITY METRIC\n",
    "def euclidean_distance(vec_1, vec_2):\n",
    "    return torch.norm(vec_1 - vec_2)\n",
    "def get_cos_sim(words, writer):\n",
    "    tokens = torch.tensor([tokenizer.tokenize(words)]).to(device)\n",
    "    vecs = writer.emb(tokens)\n",
    "    return cosine_simularity(vecs[0, 0], vecs[0, 1])\n",
    "def get_euc_dist(words, writer):\n",
    "    tokens = torch.tensor([tokenizer.tokenize(words)]).to(device)\n",
    "    vecs = writer.emb(tokens)\n",
    "    return euclidean_distance(vecs[0, 0], vecs[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "237e42d3-437f-43f0-abb0-7567b9e559b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LyricalGenius2(\n",
       "  (emb): Embedding(45509, 64)\n",
       "  (rnn): LSTM(64, 256, num_layers=3, batch_first=True)\n",
       "  (out): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=45509, bias=True)\n",
       "    (3): LogSoftmax(dim=1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writer.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "315535da-86e0-4da5-9953-2d2d3b1e89dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1.0, 'love'), (0.5264552235603333, 'hate'), (0.5208090543746948, 'forgiven'), (0.5175228714942932, 'torment'), (0.5052473545074463, 'fairyland'), (0.5013605952262878, 'less?'), (0.5011232495307922, 'sheena'), (0.4963341951370239, 'wheat'), (0.4951170086860657, 'di'), (0.4826781153678894, 'self-control')]\n"
     ]
    }
   ],
   "source": [
    "seed_word = \"love \"\n",
    "words = []\n",
    "for word in tokenizer.tokens:\n",
    "    score = get_cos_sim(seed_word + word, writer)\n",
    "    words.append((abs(score.item()), word))\n",
    "words = sorted(words, reverse=True)\n",
    "print(words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "f75606e4-e706-4968-a62b-eba7ebb65aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.0, 'love'), (9.328536987304688, 'torment'), (9.346423149108887, '(sorry'), (9.377598762512207, 'mastery'), (9.39035415649414, 'fairyland'), (9.499368667602539, 'carousels'), (9.560234069824219, 'discipline'), (9.686524391174316, 'hate'), (9.752927780151367, 'make-believe'), (9.75964069366455, 'earth)')]\n"
     ]
    }
   ],
   "source": [
    "seed_word = \"love \"\n",
    "words = []\n",
    "for word in tokenizer.tokens:\n",
    "    score = get_euc_dist(seed_word + word, writer)\n",
    "    words.append((abs(score.item()), word))\n",
    "words = sorted(words)\n",
    "print(words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1f628b-4379-4e1a-9a0f-3696a11bf219",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
